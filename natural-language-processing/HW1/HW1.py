# -*- coding: utf-8 -*-
"""HW1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bTQ5ZVpk0NM4Rzmkazwjvcux_D4iIZXF

## Part I: Data Pre-processing
"""

import pandas as pd
# Commented out because using local machine to run, the file questions-words.txt must've been exist in the folder
# Download the Google Analogy dataset
# !wget http://download.tensorflow.org/data/questions-words.txt

# Preprocess the dataset
file_name = "questions-words"
with open(f"{file_name}.txt", "r") as f:
    data = f.read().splitlines()

# check data from the first 10 entries
for entry in data[:10]:
    print(entry)

# TODO1: Write your code here for processing data to pd.DataFrame
# Please note that the first five mentions of ": " indicate `semantic`,
# and the remaining nine belong to the `syntatic` category.

# Declare the lists
questions = []
categories = []
sub_categories = []

# Current subcategory
current_sub = ""
# Counter; if count > 5 then we will use "Syntactic", else "Semantic"
count = 0

# Loop the data using index
for i in range(len(data)):
    # Check if the data in index i contains ": ", which is the subcategory.
    # If yes, assign it to the current_sub, ant plus the count with 1
    if ": " in data[i]:
      current_sub = data[i]
      count += 1
    # If not, do the tasks: assign the questions with the current index
    # And assign the sub_categories with the current_sub
    else:
      questions.append(data[i])
      sub_categories.append(current_sub)
      if count > 5:
        categories.append("Syntactic")
      else:
        categories.append("Semantic")

# Create the dataframe
df = pd.DataFrame(
    {
        "Question": questions,
        "Category": categories,
        "SubCategory": sub_categories,
    }
)

df.head()

df.to_csv(f"{file_name}.csv", index=False)

"""## Part II: Use pre-trained word embeddings
- After finish Part I, you can run Part II code blocks only.
"""

#  pip install gensim
import pandas as pd
import numpy as np
import gensim.downloader
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

data = pd.read_csv("questions-words.csv")

MODEL_NAME = "glove-wiki-gigaword-100"
# You can try other models.
# https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models

# Load the pre-trained model (using GloVe vectors here)
model = gensim.downloader.load(MODEL_NAME)
print("The Gensim model loaded successfully!")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
    # TODO2: Write your code here to use pre-trained word embeddings for getting predictions of the analogy task.
    # You should also preserve the gold answers during iterations for evaluations later.
    """ Hints
    # Unpack the analogy (e.g., "man", "woman", "king", "queen")
    # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
    # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
    # Mikolov et al., 2013: big - biggest and small - smallest
    # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
    """

    # Unpack the analogy (e.g., "man", "woman", "king", "queen")
    # Split the current analogy to an list by detecting the space
    arr = analogy.split()
    # Assign each index to the variables
    # Lower each words by using lower() to prevent error not in vocabulary
    word_a, word_b, word_c, word_d = arr[0].lower(), arr[1].lower(), arr[2].lower(), arr[3].lower()

    # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
    # Reference: https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html
    pred = model.most_similar(positive=[word_b, word_c], negative=[word_a])

    # Assign the predicted result to the preds list
    preds.append(pred[0][0])

    # Assign the 'key answer' to the golds list
    golds.append(word_d)

# Perform evaluations. You do not need to modify this block!!

def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO3: Plot t-SNE for the words in the SUB_CATEGORY `: family`
# Create new list that only contains `: family`
# Colab's suggestions help writing this code
family_df = data[data["SubCategory"] == SUB_CATEGORY]

# Just take the questions part
family_df = family_df["Question"]
arr_family = []
arr_tokens = []

# Reference: https://www.kaggle.com/code/jeffd23/visualizing-word-vectors-with-t-sne
for w in family_df:
    split = w.split()
    for i in range(len(split)):
    # To make sure the uniqueness (avoiding duplicate values)
    # If the data new (not exist in the list), append it
        if split[i].lower() not in arr_family:
            arr_family.append(split[i].lower())
            arr_tokens.append(model[split[i].lower()])

# Convert to numpy (to avoid error)
arr_tokens = np.array(arr_tokens)

# Fit to t-SNE
tsne_init = TSNE(n_components=2, random_state=0)
result = tsne_init.fit_transform(arr_tokens)

x = []
y = []
for r in result:
    x.append(r[0])
    y.append(r[1])

plt.figure(figsize=(12, 10))
for i in range(len(x)):
    plt.scatter(x[i],y[i])
    plt.annotate(arr_family[i],
                 xy=(x[i], y[i]),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom'
    )

plt.title("Word Relationships from Google Analogy Task")
# I change the order because the .show() function clear the image
plt.savefig("word_relationships.png", bbox_inches="tight")
plt.show()

"""### Part III: Train your own word embeddings

### Get the latest English Wikipedia articles and do sampling.
- Usually, we start from Wikipedia dump (https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2). However, the downloading step will take very long. Also, the cleaning step for the Wikipedia corpus ([`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus)) will take much time. Therefore, we provide cleaned files for you.
"""
# Commented out because using local machine to run, the file wiki_texts_combined.txt must've been exist in the folder
# Download the split Wikipedia files
# Each file contain 562365 lines (articles).
# gdown --id 1jiu9E1NalT2Y8EIuWNa1xf2Tw1f1XuGd -O wiki_texts_part_0.txt.gz
# gdown --id 1ABblLRd9HXdXvaNv8H9fFq984bhnowoG -O wiki_texts_part_1.txt.gz
# gdown --id 1z2VFNhpPvCejTP5zyejzKj5YjI_Bn42M -O wiki_texts_part_2.txt.gz
# gdown --id 1VKjded9BxADRhIoCzXy_W8uzVOTWIf0g -O wiki_texts_part_3.txt.gz
# gdown --id 16mBeG26m9LzHXdPe8UrijUIc6sHxhknz -O wiki_texts_part_4.txt.gz

# Download the split Wikipedia files
# Each file contain 562365 lines (articles), except the last file.
# gdown --id 17JFvxOH-kc-VmvGkhG7p3iSZSpsWdgJI -O wiki_texts_part_5.txt.gz
# gdown --id 19IvB2vOJRGlrYulnTXlZECR8zT5v550P -O wiki_texts_part_6.txt.gz
# gdown --id 1sjwO8A2SDOKruv6-8NEq7pEIuQ50ygVV -O wiki_texts_part_7.txt.gz
# gdown --id 1s7xKWJmyk98Jbq6Fi1scrHy7fr_ellUX -O wiki_texts_part_8.txt.gz
# gdown --id 17eQXcrvY1cfpKelLbP2BhQKrljnFNykr -O wiki_texts_part_9.txt.gz
# gdown --id 1J5TAN6bNBiSgTIYiPwzmABvGhAF58h62 -O wiki_texts_part_10.txt.gz

# Extract the downloaded wiki_texts_parts files.
# gunzip -k wiki_texts_part_*.gz

# Combine the extracted wiki_texts_parts files.
# cat wiki_texts_part_*.txt > wiki_texts_combined.txt

# Check the first ten lines of the combined file
# head -n 10 wiki_texts_combined.txt

"""Please note that we used the default parameters of [`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus) for cleaning the Wiki raw file. Thus, words with one character were discarded."""

# Now you need to do sampling because the corpus is too big.
# You can further perform analysis with a greater sampling ratio.

import random
# Fix the randomness, so the result not vary
random.seed(42)

wiki_txt_path = "wiki_texts_combined.txt"
# wiki_texts_combined.txt is a text file separated by linebreaks (\n).
# Each row in wiki_texts_combined.txt indicates a Wikipedia article.
output_path = "wiki_texts_sampled.txt"
sample_size = 0
sample_ratio = 0.2
with open(wiki_txt_path, "r", encoding="utf-8") as f:
    with open(output_path, "w", encoding="utf-8") as output_file:
    # TODO4: Sample `20%` Wikipedia articles
    # Write your code here
        # Reference: https://www.geeksforgeeks.org/python/python-random-sample-function/
        # Colab's suggestions help writing this code
        # This code works because my RAM (128GB) is more than enough to open and read the file
        lines = f.readlines()
        sample_size = int(sample_ratio * len(lines))
        f.seek(0)
        sampled = random.sample(lines, sample_size)
        output_file.writelines(sampled)

# TODO5: Train your own word embeddings with the sampled articles
# https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec
# Hint: You should perform some pre-processing before training.
import re
# import spacy
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

# import multiprocessing

output_path = "wiki_texts_sampled.txt"
pre_processed_path = "wiki_texts_preprocessed.txt"

with open(output_path, "r", encoding="utf-8") as f:
    with open(pre_processed_path, "w", encoding="utf-8") as output_file:
        for l in tqdm(f, total=sample_size):
            # Split the sentence
            arr_split = l.split()
            # Declare empty list
            tokens = []
            # In each iteration, check if the word ONLY contains ASCII or not
            for item in arr_split:
                if item.isascii():
                    # If yes, append to the list
                    tokens.append(item)

            # Merge all the words to a sentence, write to the txt
            sentence = " ".join(tokens)
            output_file.write(sentence)
            output_file.write("\n")

# Reference: https://www.geeksforgeeks.org/python/python-word-embedding-using-word2vec/
# Use Continuous bag of words (CBOW) model from Word2Vec
model_cbow = gensim.models.Word2Vec(sentences=LineSentence(pre_processed_path), vector_size=200, workers=32, epochs=10)
model_cbow.save("cbow_final.model")

data = pd.read_csv("questions-words.csv")

# Do predictions and preserve the gold answers (word_D)
preds = []
golds = []

for analogy in tqdm(data["Question"]):
    # TODO6: Write your code here to use your trained word embeddings for getting predictions of the analogy task.
    # You should also preserve the gold answers during iterations for evaluations later.
    """ Hints
    # Unpack the analogy (e.g., "man", "woman", "king", "queen")
    # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
    # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
    # Mikolov et al., 2013: big - biggest and small - smallest
    # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
    """
    # Split each sentences by the whitespaces
    tokens = analogy.split()

    # Same as before, assign each token (string) to word a-d, and lowercase them
    word_a, word_b, word_c, word_d = tokens[0].lower(), tokens[1].lower(), tokens[2].lower(), tokens[3].lower()

    # This try-except logic was taken from claude.ai, because i was experiencing an error:
    # Key not present in vocabulary when doing the 5% sampling, possibly because the sample is too small
    try:
        if all(word in model_cbow.wv.key_to_index for word in [word_a, word_b, word_c]):
            # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
            # Reference: https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html
            pred = model_cbow.wv.most_similar(positive=[word_b, word_c], negative=[word_a])[0][0]
        else:
            pred = "UNK"
    except:
        pred = "UNK"

    # Assign the predicted result to the preds list
    preds.append(pred)

    # Assign the 'key answer' to the golds list
    golds.append(word_d)

# Perform evaluations. You do not need to modify this block!!
import numpy as np
def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
    return np.mean(gold == pred)

golds_np, preds_np = np.array(golds), np.array(preds)
data = pd.read_csv("questions-words.csv")

# Evaluation: categories
for category in data["Category"].unique():
    mask = data["Category"] == category
    golds_cat, preds_cat = golds_np[mask], preds_np[mask]
    acc_cat = calculate_accuracy(golds_cat, preds_cat)
    print(f"Category: {category}, Accuracy: {acc_cat * 100}%")

# Evaluation: sub-categories
for sub_category in data["SubCategory"].unique():
    mask = data["SubCategory"] == sub_category
    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
    print(f"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%")

# Collect words from Google Analogy dataset
SUB_CATEGORY = ": family"

# TODO7: Plot t-SNE for the words in the SUB_CATEGORY `: family`
# Create new list that only contains `: family`
family_df = data[data["SubCategory"] == SUB_CATEGORY]

# Just take the questions part
family_df = family_df["Question"]
arr_family = []
arr_tokens = []

# Reference: https://www.kaggle.com/code/jeffd23/visualizing-word-vectors-with-t-sne
for w in family_df:
    split = w.split()
    for i in range(len(split)):
    # To make sure the uniqueness (avoiding duplicate values)
    # If the data new (not exist in the list), append it
        if split[i].lower() not in arr_family:
            arr_family.append(split[i].lower())
            arr_tokens.append(model_cbow.wv[split[i].lower()])

# Convert to numpy (to avoid error)
arr_tokens = np.array(arr_tokens)

# Fit to t-SNE
tsne_init = TSNE(n_components=2, random_state=0)
result = tsne_init.fit_transform(arr_tokens)

x = []
y = []
for r in result:
    x.append(r[0])
    y.append(r[1])

plt.figure(figsize=(12, 10))
for i in range(len(x)):
    plt.scatter(x[i],y[i])
    plt.annotate(arr_family[i],
                 xy=(x[i], y[i]),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom'
    )

plt.title("Word Relationships from Google Analogy Task")
# I change the order because the .show() function clear the image
plt.savefig("word_relationships_ownmodel.png", bbox_inches="tight")
plt.show()