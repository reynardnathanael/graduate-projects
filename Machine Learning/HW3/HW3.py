# -*- coding: utf-8 -*-
"""HW3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-9qpuDyiWUugC9o8rjEQFWymUvmcJ6ge
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Processing the data from the csv files
def process_data():
  # Read csv
  exercises = pd.read_csv("exercise.csv")
  calories = pd.read_csv("calories.csv")

  # Merge csv by User_ID, one hot encoding gender, drop User_ID
  data = pd.merge(exercises, calories, on="User_ID")
  data["Gender"] = data["Gender"].map({"male": 1, "female": 0})
  data.drop(columns=["User_ID"], inplace=True)

  # Define X (features) and Y (target)
  X = data.drop(columns=["Calories"])
  Y = data["Calories"]

  # Random data, split into train, validate, and test (70:10:20)
  np.random.seed(42)
  shuffle = np.random.permutation(X.values.shape[0])

  X_shuffled = X.iloc[shuffle].reset_index(drop=True)
  Y_shuffled = Y.iloc[shuffle].reset_index(drop=True)

  # Define the split percentages
  train_percent = int(0.7 * len(X))
  validate_percent = int(0.8 * len(X))

  # Splitting data into train, validate, test
  X_train = X_shuffled.iloc[:train_percent]
  X_validate = X_shuffled.iloc[train_percent:validate_percent]
  X_test = X_shuffled.iloc[validate_percent:]

  Y_train = Y_shuffled.iloc[:train_percent]
  Y_validate = Y_shuffled.iloc[train_percent:validate_percent]
  Y_test = Y_shuffled.iloc[validate_percent:]

  return X_train, Y_train, X_validate, Y_validate, X_test, Y_test

# Expanding features (Adapting from sklearn's PolynomialFeatures)
def polynomial_expansion(X):
  arr_expansion = []
  # Iterating through each row in X
  for row in X.values:
    arr_features = []
    # Add constant value (1)
    arr_features.append(1)
    # Adding the original features
    arr_features.extend(row)

    # Iterating through each feature
    for i in range(len(row)):
      # Iterating from i to row's length (avoid redundant calculations i * j)
      for j in range(i, len(row)):
        # Calculates the interaction between i-th and j-th features
        interaction = row[i] * row[j]
        arr_features.append(interaction)

    # Add the expanded feature
    arr_expansion.append(arr_features)

  return np.array(arr_expansion)

# Initiating matrix + bias (if not using polynomial expansion)
def initiate_matrix(X):
  # If X not a numpy array, change it to numpy array
  X = np.array(X) if isinstance(X, np.ndarray) == False else X
  arr_result = []

  # Iterate through every row in X
  for row in X:
    # Add 1 at the beginning (representing the bias term)
    new = [1] + list(row)
    arr_result.append(new)

  return np.array(arr_result)

# Maximum Likelihood Estimation / Ordinary Least Square (OLS)
def MLR(X_train, t, X_test, expand=True):
  # Define the PHI and X_test
  phi = polynomial_expansion(X_train) if expand else initiate_matrix(X_train)
  X_test = polynomial_expansion(X_test) if expand else initiate_matrix(X_test)

  # Calculate the weight
  w = np.linalg.pinv(phi).dot(t)
  # Calculate y(x,w)
  y_x_w = X_test.dot(w)
  return y_x_w, w

# Bayesian Linear Regression
def BLR(X_train, t, X_validate, alpha, beta, expand=True):
  # Define the PHI and X_test using Polynomial Features
  phi = polynomial_expansion(X_train) if expand else initiate_matrix(X_train)
  X_validate = (polynomial_expansion(X_validate) if expand
                else initiate_matrix(X_validate))

  # Calculate SN^-1
  S_N_inverse = (alpha * np.identity(phi.shape[1])) + beta * np.dot(phi.T, phi)
  # Calculate mN
  m_N = beta * np.dot(np.linalg.inv(S_N_inverse), np.dot(phi.T, t))

  # Calculate y(x,w)
  y_x_w = np.dot(X_validate, m_N)
  return y_x_w, m_N, np.linalg.inv(S_N_inverse)

# Mean Squared Error
def mean_squared_error(y, y_hat):
  return np.mean((y - y_hat) ** 2)

# Plotting the best-fit lines for both models
def plot_best_fit_lines(X_train, Y_train, X_validate, Y_validate, types):
  # Select only the "Duration" column
  X_train_duration = X_train[['Duration']]
  X_validate_duration = X_validate[['Duration']]

  # Doing the MLR and BLR
  y_mlr, w_mlr = MLR(X_train_duration, Y_train, X_validate_duration, False)
  y_blr, m_N, S_N = BLR(X_train_duration, Y_train, X_validate_duration,
                        0.01, 0.01, False)

  # Posterior samples
  np.random.seed(42)
  w_samples = np.random.multivariate_normal(m_N, S_N, 100)

  # Plot the graph
  plt.figure(figsize=(8, 8))

  # Iterating through the weights (100)
  for i in range(len(w_samples)):
    # Calculate y(x,w)
    y_blr_sample = np.dot(initiate_matrix(X_validate_duration), w_samples[i])
    # Plot the red line (Bayesian)
    plt.plot(X_validate_duration, y_blr_sample, linewidth=1, color='red',
             alpha=0.8, label='Bayesian Posterior Fits' if i == 99 else None)

  # Plot the black line (OLS)
  plt.plot(X_validate_duration, y_mlr, 'k--', linewidth=1, label='OLS Fit')
  # Plot the blue dots (Observations)
  plt.scatter(X_train_duration, Y_train, s=12, alpha=0.8, color='blue',
              label='Observations')

  # Add some properties for the graph
  plt.title('Posterior Predictions with ' + types + ' Observations',
            fontsize=20)
  plt.xlabel('Duration (min)', fontsize=18)
  plt.ylabel('Calories', fontsize=18);
  plt.legend(fontsize=16)
  plt.show()

# Main function
def main():
  # Process the data (read, shuffle, split)
  X_train, Y_train, X_validate, Y_validate, X_test, Y_test = process_data()

  # Implement the MLR and calculate MSE for the test set
  y_mlr, w_mlr = MLR(X_train, Y_train, X_test)
  mse_mlr = mean_squared_error(Y_test, y_mlr)
  print(f"Maximum Likelihood Estimation (Test Set)")
  print(f"MSE: {mse_mlr:.4f}")

  print()

  # Using just 500 training data for limited observations
  X_train_limited = X_train[:500]
  Y_train_limited = Y_train[:500]

  # Implement the BLR and calculate MSE for the validation set (limited observations)
  y_blr_limited, w_blr_limited, S_N_blr_limited = BLR(X_train_limited,
                            Y_train_limited, X_validate, 1e-3, 1e2)
  mse_blr_limited = mean_squared_error(Y_validate, y_blr_limited)

  print(f"Bayesian Linear Regression with Limited Observations (Validation Set)")
  print(f"MSE: {mse_blr_limited:.4f}")

  print()

  # Implement the BLR and calculate MSE for the validation set (full observations)
  y_blr, w_blr, S_N_blr = BLR(X_train, Y_train, X_validate, 1e-3, 1e2)
  mse_blr = mean_squared_error(Y_validate, y_blr)
  print(f"Bayesian Linear Regression with All Observations (Validation Set)")
  print(f"MSE: {mse_blr:.4f}")

  # Plotting the graph (full observations)
  plot_best_fit_lines(X_train, Y_train, X_test, Y_test, 'All')
  print()
  # Plotting the graph (limited observations)
  plot_best_fit_lines(X_train_limited, Y_train_limited, X_test, Y_test,
                      'Limited')

if __name__ == "__main__":
  main()