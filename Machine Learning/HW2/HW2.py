# -*- coding: utf-8 -*-
"""HW2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gTkCO8XMCrIQiQvCJmX5m0kVR2yv2tEi
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import seaborn as sns
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt
import os
import random
from PIL import Image

np.random.seed(42)
random.seed(42)
torch.manual_seed(42)

# Dimensionality reduction using PCA
USE_PCA = True
if USE_PCA:
    NUM_COMPONENTS = 2
    pca = PCA(n_components=NUM_COMPONENTS)

# Processing the images into datas
class FruitDataset(Dataset):
  def __init__(self, data_dir, split, use_pca=True):
    assert split in ['train', 'validation', 'test']

    # Declare self variables
    self.split = split
    self.data_dir = os.path.join(data_dir, 'Data_train' if self.split == 'train' or self.split == 'validation' else 'Data_test')
    self.data_types = ['Carambula', 'Lychee', 'Pear']
    self.classes = {'Carambula': 0, 'Lychee': 1, 'Pear': 2}

    # Arrays for images and labels
    self.images = []
    self.labels = []

    # If train or validation, load the images and labels, shuffles,
    # and split it into training (80% - 1176) and validation (20% - 294) sets
    if split == 'train' or split == 'validation':
      arr_images = []
      arr_labels = []

      # Generates file paths (e.g. Carambula_train_0.png)
      for data_type in self.data_types:
        index = self.classes[data_type]
        paths = []
        for i in range(490):
            paths.append(str(data_type) + "_train_" + str(i) + ".png")

        # Converting images into numpy array
        for path in paths:
          cur_path = os.path.join(self.data_dir, data_type, path)
          image = np.array(Image.open(cur_path), dtype=np.float32)[..., 0] / 255.
          arr_images.append(image)
          arr_labels.append(index)

      train_images = []
      train_labels = []
      validation_images = []
      validation_labels = []

      # Splitting data to training and validation sets
      for j in range(len(self.data_types)):
        start = 490 * j
        end = 490 + start

        class_images = arr_images[start:end]
        class_labels = arr_labels[start:end]

        index = list(range(490))
        random.shuffle(index)

        partition_rate = int(0.8 * 490)

        for k in index[:partition_rate]:
          train_images.append(class_images[k])
          train_labels.append(class_labels[k])

        for l in index[partition_rate:]:
          validation_images.append(class_images[l])
          validation_labels.append(class_labels[l])

      if split == 'train':
        self.images = train_images
        self.labels = train_labels
      else:
        self.images = validation_images
        self.labels = validation_labels

    # For test data, no need to split the images
    else:
      for data_type in self.data_types:
        index = self.classes[data_type]
        paths = []
        for i in range(166):
            paths.append(str(data_type) + "_test_" + str(i) + ".png")

        for path in paths:
          cur_path = os.path.join(self.data_dir, data_type, path)
          image = np.array(Image.open(cur_path), dtype=np.float32)[..., 0] / 255.
          self.images.append(image)
          self.labels.append(index)

    self.images = np.array(self.images)
    self.labels = np.array(self.labels)
    print(f"# of images is {len(self.images)}")

    # Call the PCA function to do the dimensionality reduction
    if use_pca:
      self.images_pca = self.get_PCA_features()

  def get_PCA_features(self):
    images_reshape = self.images.reshape(self.images.shape[0], -1)
    if self.split == 'train':
      return pca.fit_transform(images_reshape)
    else:
      return pca.transform(images_reshape)

  # working for indexing
  def __getitem__(self, index):
    if USE_PCA:
      return np.append(self.images_pca[index], 1), self.labels[index]
    else:
      return np.append(self.images[index], 1), self.labels[index]

  # return the length of our dataset
  def __len__(self):
    return len(self.images)

# Set up the dataset by calling the class for each data type (train, val, test)
training_data = FruitDataset('Data', 'train')
validation_data = FruitDataset('Data', 'validation')
test_data = FruitDataset('Data', 'test')

# Check one of the image
image_pca_features, label = training_data[0]
image = training_data.images[0]
plt.imshow(image, cmap='gray', vmin=0, vmax=1)
print("Label: " + str(training_data.data_types[label]))

# Declare images data and labels for each data types
pca_train_df = training_data.images_pca
pca_train_labels = training_data.labels

pca_validation_df = validation_data.images_pca
pca_validation_labels = validation_data.labels

pca_test_df = test_data.images_pca
pca_test_labels = test_data.labels

# Declare figure and axes for 3 visualization
figure, axes = plt.subplots(1, 3, figsize=(15, 5))

pca_arr_set = [[pca_train_df, pca_train_labels], [pca_validation_df, pca_validation_labels], [pca_test_df, pca_test_labels]]
type_name = ['Training', 'Validation', 'Test']

# Iterate for each dataset type
for j in range (len(pca_arr_set)):
  # Plot the PCA-Transformed data
  for i, label in zip(range(len(training_data.data_types)), training_data.data_types):
    axes[j].scatter(pca_arr_set[j][0][(pca_arr_set[j][1] == i), 0], pca_arr_set[j][0][(pca_arr_set[j][1] == i), 1], label=label, alpha=0.7)

  axes[j].set_title("PCA-Transformed " + type_name[j] + " Data")
  axes[j].legend()
  axes[j].grid(True)
  axes[j].set_xlabel("Principal Component 1")
  axes[j].set_ylabel("Principal Component 2")

plt.tight_layout()
plt.show()

# Process the data from the FruitDataset class
def process_images_data(data):
  images = []
  labels = []

  for img, label in data:
      # Add the flatten images and the labels to the list
      images.append(img.flatten())
      labels.append(label)

  # Change it to the numpy array form
  img = np.array(images).T
  lbl = np.array(labels).reshape(1, -1)

  return img, lbl

# One Hot Encoding
def one_hot_encoding(Y_train, Y_validation, Y_test, output_size):
  Y_train_onehot = np.eye(output_size)[Y_train.flatten()].T
  Y_validation_onehot = np.eye(output_size)[Y_validation.flatten()].T
  Y_test_onehot = np.eye(output_size)[Y_test.flatten()].T
  return Y_train_onehot, Y_validation_onehot, Y_test_onehot

# Define the activation functions
def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))
    return exp_x / np.sum(exp_x, axis=0, keepdims=True)

def relu(x):
  return np.maximum(0, x)

def relu_derivative(x):
  return (x > 0).astype(float)

# Define global variables to store datas
layers = {}
first_moment_m = {}
second_moment_v = {}

# Preparing layers before initiating the model
def prepare_layer(input_layer, hidden_layer, output_layer, hidden_layer2=False):
  layers.clear()
  first_moment_m.clear()
  second_moment_v.clear()

  # For 2-layer (w1, w2, b1, b2)
  if hidden_layer2 == False:
    np.random.seed(42)
    w1 = np.random.randn(hidden_layer, input_layer) * np.sqrt(2.0 / input_layer)
    w2 = np.random.randn(output_layer, hidden_layer) * np.sqrt(2.0 / hidden_layer)
    b1 = np.zeros((hidden_layer, 1))
    b2 = np.zeros((output_layer, 1))
    layers['w1'] = w1
    layers['w2'] = w2
    layers['b1'] = b1
    layers['b2'] = b2

  # For 3-layer (w1, w2, w3, b1, b2, b3)
  else:
    np.random.seed(42)
    w1 = np.random.randn(hidden_layer, input_layer) * np.sqrt(2.0 / input_layer)
    w2 = np.random.randn(hidden_layer2, hidden_layer) * np.sqrt(2.0 / hidden_layer)
    w3 = np.random.randn(output_layer, hidden_layer2) * np.sqrt(2.0 / hidden_layer2)
    b1 = np.zeros((hidden_layer, 1))
    b2 = np.zeros((hidden_layer2, 1))
    b3 = np.zeros((output_layer, 1))
    layers['w1'] = w1
    layers['w2'] = w2
    layers['w3'] = w3
    layers['b1'] = b1
    layers['b2'] = b2
    layers['b3'] = b3

  # Initiate Adam variables
  for key, value in layers.items():
    first_moment_m[key] = np.zeros_like(value)
    second_moment_v[key] = np.zeros_like(value)

# Forward Propagation
def forward_propagation(x, layer3=False):
  # For 2-layer (compute z1, a1, z2, a2)
  if layer3 == False:
    z1 = np.dot(layers['w1'], x) + layers['b1']
    a1 = relu(z1)
    z2 = np.dot(layers['w2'], a1) + layers['b2']
    a2 = softmax(z2)
    return { 'a1': a1, 'z1': z1, 'a2': a2, 'z2': z2 }

  # For 3-layer (compute z1, a1, z2, a2, z3, a3)
  else:
    z1 = np.dot(layers['w1'], x) + layers['b1']
    a1 = relu(z1)
    z2 = np.dot(layers['w2'], a1) + layers['b2']
    a2 = relu(z2)
    z3 = np.dot(layers['w3'], a2) + layers['b3']
    a3 = softmax(z3)
    return { 'a1': a1, 'z1': z1, 'a2': a2, 'z2': z2, 'a3': a3, 'z3': z3 }

# Optimizer #1 : Stochastic Gradient Descent (SGD)
def stochastic_gradient_descent(data, learning_rate, layer3=False):
  for key, value in data.items():
    layers[key] = layers[key] - (learning_rate * data[key])

# Optimizer #2 : Adaptive Moment Estimation (ADAM)
def adaptive_moment_estimation(data, order):
  beta1 = 0.9
  beta2 = 0.999
  epsilon = 1e-8
  for key, value in layers.items():
    first_moment_m[key] = (beta1 * first_moment_m[key]) + ((1 - beta1) * data[key])
    second_moment_v[key] = (beta2 * second_moment_v[key]) + ((1 - beta2) * (data[key] ** 2))

    m_hat = first_moment_m[key] / (1 - (beta1 ** (order + 1)))
    v_hat = second_moment_v[key] / (1 - (beta2 ** (order + 1)))

    layers[key] = layers[key] - (learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon)))

# Back Propagation
def back_propagation(x, y, forward, learning_rate, optimizer, order, layer3=False):
  # Define 'N' (in this code we use 'm' as a common notation)
  m = x.shape[1]

  # For 2-layer (compute dz2, dw2, db2, da1, dw1, db1)
  if layer3 == False:
    dz2 = forward['a2'] - y
    dw2 = (1 / m) * np.dot(dz2, forward['a1'].T)
    db2 = (1 / m) * np.sum(dz2, axis=1, keepdims=True)

    dz1 = np.dot(layers['w2'].T, dz2) * relu_derivative(forward['z1'])
    dw1 = (1 / m) * np.dot(dz1, x.T)
    db1 = (1 / m) * np.sum(dz1, axis=1, keepdims=True)

    # Data for optimizer's parameter
    data = { 'w1': dw1, 'b1': db1, 'w2': dw2, 'b2': db2 }
    if (optimizer == 'SGD'):
      stochastic_gradient_descent(data, learning_rate, layer3)
    elif (optimizer == 'ADAM'):
      adaptive_moment_estimation(data, order)

  # For 3-layer (compute dz3, dw3, db3, dz2, dw2, db2, dz1, dw1, db1)
  else:
    dz3 = forward['a3'] - y
    dw3 = (1 / m) * np.dot(dz3, forward['a2'].T)
    db3 = (1 / m) * np.sum(dz3, axis=1, keepdims=True)

    dz2 = np.dot(layers['w3'].T, dz3) * relu_derivative(forward['z2'])
    dw2 = (1 / m) * np.dot(dz2, forward['a1'].T)
    db2 = (1 / m) * np.sum(dz2, axis=1, keepdims=True)

    dz1 = np.dot(layers['w2'].T, dz2) * relu_derivative(forward['z1'])
    dw1 = (1 / m) * np.dot(dz1, x.T)
    db1 = (1 / m) * np.sum(dz1, axis=1, keepdims=True)

    # Data for optimizer's parameter
    data = { 'w1': dw1, 'b1': db1, 'w2': dw2, 'b2': db2, 'w3': dw3, 'b3': db3 }
    if (optimizer == 'SGD'):
      stochastic_gradient_descent(data, learning_rate, layer3)
    elif (optimizer == 'ADAM'):
      adaptive_moment_estimation(data, order)

# Computing loss using cross-entropy loss
def compute_loss(y_hat, y):
  epsilon = 1e-10
  m = y.shape[1]
  y_hat = np.clip(y_hat, epsilon, 1 - epsilon)
  return -np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) / m

# Calculate the accuracy
def predict(predictions, y_true, layer3):
  pred_labels = np.argmax(predictions['a2'], axis=0) if layer3 == False else np.argmax(predictions['a3'], axis=0)
  return np.mean(y_true.flatten() == pred_labels)

# Plotting the loss curves
def plot_loss_curves(train, validation, test):
  figure, axes = plt.subplots(1, 3, figsize=(15, 5))
  dataset_type = ['Training', 'Validation', 'Test']
  colors = ['blue', 'red', 'green']

  for i in range(len(dataset_type)):
    selected_loss = train if i == 0 else validation if i == 1 else test
    axes[i].plot(selected_loss, label=(dataset_type[i] + " Loss"), color=colors[i])
    axes[i].set_title(dataset_type[i] + " Loss Curves")
    axes[i].set_xlabel("Epochs")
    axes[i].set_ylabel("Loss")
    axes[i].legend()

  plt.tight_layout()
  plt.show()

def plot_decision_regions(x_train, y_train, x_validation, y_validation, x_test, y_test, model_forward, layer3):
  h = 0.01
  figure, axes = plt.subplots(1, 3, figsize=(15, 5))

  arr_data = [[x_train, y_train], [x_validation, y_validation], [x_test, y_test]]

  for i in range(len(arr_data)):
    x_min, x_max = arr_data[i][0][0, :].min() - 1, arr_data[i][0][0, :].max() + 1
    y_min, y_max = arr_data[i][0][1, :].min() - 1, arr_data[i][0][1, :].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    # Flatten and add bias term
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    grid_points_bias = np.c_[grid_points, np.ones(grid_points.shape[0])].T

    # Predict
    predictions = model_forward(grid_points_bias, layer3)
    Z = np.argmax(predictions['a2'], axis=0) if layer3 == False else np.argmax(predictions['a3'], axis=0)
    Z = Z.reshape(xx.shape)

    axes[i].contourf(xx, yy, Z, alpha=0.7, cmap=plt.cm.RdYlBu)

    # Plot the real data
    for j, label in zip(range(len(training_data.data_types)), training_data.data_types):
        axes[i].scatter(
            arr_data[i][0][0, arr_data[i][1].flatten() == j],
            arr_data[i][0][1, arr_data[i][1].flatten() == j],
            label=label, edgecolor='k', s=40
        )

    axes[i].set_xlabel('PC 1')
    axes[i].set_ylabel('PC 2')
    title = 'Decision Region - Training' if i == 0 else 'Decision Region - Validation' if i == 1 else 'Decision Region - Test'
    axes[i].set_title(title)
    axes[i].legend()
    axes[i].grid(True)

  plt.tight_layout()
  plt.show()

# Train the Network
def train_network(x_train, x_validation, x_test, y_train, y_validation, y_test, learning_rate, epochs, optimizer, output_size, layer3=False):
  arr_train_loss = []
  arr_validation_loss = []
  arr_test_loss = []
  y_train_onehot, y_validation_onehot, y_test_onehot = one_hot_encoding(y_train, y_validation, y_test, output_size)

  # Initiating variables for early stopping
  largest_loss = float('inf')
  verbose = 20
  monitor = 0
  # Iterate based on the epochs' size (1000)
  for i in range(epochs):
    # Forward propagation for training dataset
    y_train_hat = forward_propagation(x_train, layer3)
    # Calculate the training accuracy rate
    train_accuracy = predict(y_train_hat, y_train, layer3)
    # Compute training's loss
    train_loss = compute_loss(y_train_hat['a3' if layer3 == True else 'a2'], y_train_onehot)

    # Forward propagation for validation dataset
    y_validation_hat = forward_propagation(x_validation, layer3)
    # Calculate the validation accuracy rate
    validation_accuracy = predict(y_validation_hat, y_validation, layer3)
    # Compute validation's loss
    validation_loss = compute_loss(y_validation_hat['a3' if layer3 == True else 'a2'], y_validation_onehot)

    # Early stopping check
    if validation_loss < largest_loss:
        largest_loss = validation_loss
        monitor = 0
    else:
        monitor += 1

    # Executing early stopping
    if monitor >= verbose:
        print(f"Early stopping at epoch {i+1}")
        print(
            f"Epoch {i+1}, "
            f"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, "
            f"Val Loss: {validation_loss:.4f}, Val Acc: {validation_accuracy:.4f}, "
            f"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}"
        )
        print(f"Test Accuracy: {test_accuracy * 100:.2f}%")
        break

    # Do the backpropagation
    back_propagation(x_train, y_train_onehot, y_train_hat, learning_rate, optimizer, i, layer3)

    # Forward propagation for test dataset
    y_test_hat = forward_propagation(x_test, layer3)
    # Calculate the test accuracy rate
    test_accuracy = predict(y_test_hat, y_test, layer3)
    # Compute test's loss
    test_loss = compute_loss(y_test_hat['a3' if layer3 == True else 'a2'], y_test_onehot)

    # Save the loss result to each array
    arr_train_loss.append(train_loss)
    arr_validation_loss.append(validation_loss)
    arr_test_loss.append(test_loss)

    # Print the result 10x
    # 1000 epochs -> 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000
    if (i+1) % 100 == 0:
      print(
          f"Epoch {i+1}, "
          f"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, "
          f"Val Loss: {validation_loss:.4f}, Val Acc: {validation_accuracy:.4f}, "
          f"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}"
      )
      if (i == epochs - 1):
        print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

  # Plot loss curves and decision regions
  plot_loss_curves(arr_train_loss, arr_validation_loss, arr_test_loss)
  plot_decision_regions(x_train, y_train, x_validation, y_validation, x_test, y_test, forward_propagation, layer3)

# Declare the training, validation, and test data
X_train, Y_train = process_images_data(training_data)
X_validation, Y_validation = process_images_data(validation_data)
X_test, Y_test = process_images_data(test_data)

# Set the size of the input, hidden, and output layer
input_size = 3
hidden_size = 6
output_size = 3

# Set the learning rate
learning_rate = 0.01
# Set the epochs
epochs = 1000

# Prepare the layer
prepare_layer(input_size, hidden_size, output_size)

# Choose one of the optimizer
optimizer = 'SGD'
# optimizer = 'ADAM'

# Train the Network (using 2-layer)
nn = train_network(X_train, X_validation, X_test, Y_train, Y_validation, Y_test, learning_rate, epochs, optimizer, output_size)

# Set the size of the hidden layer 2
hidden_size2 = 10

# Prepare the layer
prepare_layer(input_size, hidden_size, output_size, hidden_size2)

# Train the Network (using 3-layer)
nn = train_network(X_train, X_validation, X_test, Y_train, Y_validation, Y_test, learning_rate, epochs, optimizer, output_size, True)