# -*- coding: utf-8 -*-
"""HW1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FhvqL65Y7nUuvT8hz2UXx1zjhTScaUWj
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv('Wine.csv')
df.head()

# Groupping by target
df_group = df.groupby('target')

# Randomly select 20 instances from each category as test data
df_test = df_group.apply(lambda x: x.sample(20)).reset_index(drop=True)

# Creating the training data by removing df_test from the main data
df_train = df.drop(df_test.index)

# Save the test and training datasets
df_test.to_csv('test.csv', index=False)
df_train.to_csv('train.csv', index=False)

# Separate the training data to feature and target
feature_train = df_train.drop(columns=['target'])
target_train = df_train['target']

# Separate the test data to feature and target
feature_test = df_test.drop(columns=['target'])
target_test = df_test['target']

# Calculating Priors -> P(y) = count of class y/total samples
priors = target_train.value_counts(normalize=True)

# Define Means and Variances -> used to model Gaussian likelihoods P(x|y)
means = feature_train.groupby(target_train).mean()
variances = feature_train.groupby(target_train).var()

target_prediction = []

# Iteration 1: Feature Test
for i in range(len(feature_test)):
  # Declare empty set for the posteriors
  posteriors = {}

  # Iteration 2: loop for every index of priors
  for j in priors.index:

    # Calculate the log-prior -> log P(y)
    log_prior = np.log(priors[j])

    # Declare the parameter (x), mean (μ), and variance (σ²)
    x = feature_test.iloc[i]
    mean = means.loc[j]
    variance = variances.loc[j]

    # Calculate the log-likelihood -> log P(x | y)
    log_likelihood = np.sum(((-1 / (2 * variance)) * (x - mean) * (x - mean)) - ((1 / 2) * np.log(variance)) - ((1 / 2) * np.log(2 * np.pi)))

    # Calculate the log-posterior -> (log P(y | x) ∝ log P(x | y) + log P(y))
    log_posterior = log_likelihood + log_prior
    posteriors[j] = log_posterior

  # Search for the maximum posteriors probability
  target_prediction.append(max(posteriors, key=posteriors.get))

# Calculate the accuracy
accuracy = np.mean(np.array(target_prediction) == target_test)
print("Accuracy Rate: " + str(accuracy * 100) + "%")

# Initialize scaler
scaler = StandardScaler()

# Fit the scaler to the training features and transform the test features
feature_train_scaled = scaler.fit_transform(feature_train)
feature_test_scaled = scaler.transform(feature_test)

# Define PCA with 2 components (2D)
pca = PCA(n_components=2)

# Fit transform the scaled training features and the scaled test features
feature_train_pca = pca.fit_transform(feature_train_scaled)
feature_test_pca = pca.fit_transform(feature_test_scaled)

# Visualizing PCA with the fit-transformed scaled test features
pca_df = pd.DataFrame(feature_test_pca, columns=['PC1', 'PC2'])

# Set the target and predicted index with the target test and target prediction
pca_df['target'] = np.array(target_test)
pca_df['predicted'] = np.array(target_prediction)

# Configure the colors of the plot (red, green, blue)
colors = ["r", "g", "b"]

# Configure the labels of each target (wine 0, wine 1, wine 2)
labels = ['wine 0', 'wine 1', 'wine 2']

# Plot a figure
plt.figure(figsize=(8, 6))

# Iteration for plotting each plot
for i, color, label in zip(np.unique(df['target']), colors, labels):
    plt.scatter(pca_df.loc[pca_df['predicted'] == i, 'PC1'], pca_df.loc[pca_df['predicted'] == i, 'PC2'], color=color, label=label)

# Adding labels and title, show the legend
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("Principal Component Analysis")
plt.legend(title="Targets")

# Show the scatter plot
plt.show()

# Define a confusion matrix using 2 parameters: the target test and the target prediction
cm = confusion_matrix(target_test, target_prediction)

# Plot a figure
plt.figure(figsize=(8, 6))

# Initialize a heatmap, customize the x and y with 'wine'
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['wine 0', 'wine 1', 'wine 2'], yticklabels=['wine 0', 'wine 1', 'wine 2'])

# Add some properties (title, xlabel, ylabel)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# Show the confusion matrix
plt.show()